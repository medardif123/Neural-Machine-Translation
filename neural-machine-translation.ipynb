{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Neural machine Translation (english to french) using the attention mechanism**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T10:57:32.18028Z","iopub.execute_input":"2021-12-06T10:57:32.180728Z","iopub.status.idle":"2021-12-06T10:57:37.233826Z","shell.execute_reply.started":"2021-12-06T10:57:32.180606Z","shell.execute_reply":"2021-12-06T10:57:37.233088Z"}}},{"cell_type":"markdown","source":"**Import the libraries**","metadata":{}},{"cell_type":"code","source":"import string\nimport re\nfrom pickle import dump\nfrom unicodedata import normalize\nimport numpy as np\nfrom numpy import array\nfrom pickle import load\nfrom pickle import dump\nfrom numpy.random import rand\nfrom numpy.random import shuffle\nfrom numpy import array\nfrom numpy import argmax\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Bidirectional, Embedding,Concatenate, Permute, Dot, Input, LSTM, Multiply\nfrom tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\nfrom tensorflow.keras.models import load_model, Model","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:15:55.729445Z","iopub.execute_input":"2021-12-08T21:15:55.730243Z","iopub.status.idle":"2021-12-08T21:16:01.001590Z","shell.execute_reply.started":"2021-12-08T21:15:55.730143Z","shell.execute_reply":"2021-12-08T21:16:01.000830Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Load dataset and preprocess it (delete ponctuation and special characters,convert to lowercase)","metadata":{}},{"cell_type":"code","source":"def load_document(filename):\n    file = open(filename, mode='r', encoding='utf-8')\n    text = file.read()\n    file.close()\n    return text\n\ndef split_text_to_pairs(text):\n    lines = text.strip().split('\\n')\n    pairs = []\n    for line in lines:\n        index = line.find('CC-BY')\n        new_line = line[:index-1]\n        eng,fr = new_line.split('\\t')\n        pairs.append((eng,fr))\n    return pairs\n\ndef preprocess(lines):\n    preprocessed = list()\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    table = str.maketrans('', '', string.punctuation)\n    for pair in lines:\n        clean_pair = list()\n        for line in pair:\n            line = normalize('NFD', line).encode('ascii', 'ignore')\n            line = line.decode('UTF-8')\n            line = line.split()\n            cleaned_sentence = ''\n            for word in line :\n                word = word.lower()\n                word = re.sub(r'([!.?])', r' \\1', word)\n                word = re.sub(r'[^a-zA-Z.!?]+', r' ', word)\n                word = word.translate(table)\n                word = re.sub(r'\\s+', r' ', word)\n                cleaned_sentence += \" \" + word\n            clean_pair.append(''.join(cleaned_sentence))\n        preprocessed.append(clean_pair)\n    return array(preprocessed)\n\n\ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:01.003261Z","iopub.execute_input":"2021-12-08T21:16:01.003510Z","iopub.status.idle":"2021-12-08T21:16:01.013603Z","shell.execute_reply.started":"2021-12-08T21:16:01.003476Z","shell.execute_reply":"2021-12-08T21:16:01.012967Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"filename = '../input/fraeng/fra.txt'\ndoc = load_document(filename)\npairs = split_text_to_pairs(doc)\nclean_pairs = preprocess(pairs)\nsave_clean_data(clean_pairs, 'english-french.pkl')\nfor i in range(100):\n\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:01.014804Z","iopub.execute_input":"2021-12-08T21:16:01.017304Z","iopub.status.idle":"2021-12-08T21:16:21.278486Z","shell.execute_reply.started":"2021-12-08T21:16:01.017251Z","shell.execute_reply":"2021-12-08T21:16:21.277801Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n \ndef save_clean_data(sentences, filename):\n    dump(sentences, open(filename, 'wb'))\n    print('Saved: %s' % filename)\nraw_dataset = load_clean_sentences('english-french.pkl')\nn_sentences = 15500\ndataset = raw_dataset[:n_sentences, :]\nshuffle(dataset)\ntrain = dataset\ntest = dataset[15400:]\nsave_clean_data(dataset, 'english-french-both.pkl')\nsave_clean_data(train, 'english-french-train.pkl')\nsave_clean_data(test, 'english-french-test.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:21.280442Z","iopub.execute_input":"2021-12-08T21:16:21.281393Z","iopub.status.idle":"2021-12-08T21:16:22.133008Z","shell.execute_reply.started":"2021-12-08T21:16:21.281362Z","shell.execute_reply":"2021-12-08T21:16:22.132259Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_clean_sentences(filename):\n    return load(open(filename, 'rb'))\n \ndataset = load_clean_sentences('english-french-both.pkl')\ntrain = load_clean_sentences('english-french-train.pkl')\ntest = load_clean_sentences('english-french-test.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:22.134517Z","iopub.execute_input":"2021-12-08T21:16:22.134792Z","iopub.status.idle":"2021-12-08T21:16:22.251336Z","shell.execute_reply.started":"2021-12-08T21:16:22.134757Z","shell.execute_reply":"2021-12-08T21:16:22.250536Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)\n\neng_tokenizer = create_tokenizer(dataset[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(dataset[:, 0])\nprint('English Vocabulary Size: %d' % eng_vocab_size)\nprint('English Max Length: %d' % (eng_length))\nfr_tokenizer = create_tokenizer(dataset[:, 1])\nfr_vocab_size = len(fr_tokenizer.word_index) + 1\nfr_length = max_length(dataset[:, 1])\nprint('French Vocabulary Size: %d' % fr_vocab_size)\nprint('french Max Length: %d' % (fr_length))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:22.252817Z","iopub.execute_input":"2021-12-08T21:16:22.253074Z","iopub.status.idle":"2021-12-08T21:16:22.805812Z","shell.execute_reply.started":"2021-12-08T21:16:22.253040Z","shell.execute_reply":"2021-12-08T21:16:22.805104Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"tokenize the text and encode the output with one hot encoding","metadata":{}},{"cell_type":"code","source":"def encode_sequences(tokenizer, length, lines):\n    X = tokenizer.texts_to_sequences(lines)\n    X = pad_sequences(X, maxlen=length, padding='post')\n    return X\n\ndef encode_output(sequences, vocab_size):\n    ylist = list()\n    for sequence in sequences:\n        encoded = to_categorical(sequence, num_classes=vocab_size)\n        ylist.append(encoded)\n    y = array(ylist)\n    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    return y","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:22.807102Z","iopub.execute_input":"2021-12-08T21:16:22.807527Z","iopub.status.idle":"2021-12-08T21:16:22.814398Z","shell.execute_reply.started":"2021-12-08T21:16:22.807490Z","shell.execute_reply":"2021-12-08T21:16:22.813622Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainX = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\ntrainY = encode_sequences(fr_tokenizer, fr_length, train[:, 1])\ntrainY = encode_output(trainY, fr_vocab_size)\ntestX = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\ntestY = encode_sequences(fr_tokenizer, fr_length, test[:, 1])\ntestY = encode_output(testY, fr_vocab_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:22.815869Z","iopub.execute_input":"2021-12-08T21:16:22.816184Z","iopub.status.idle":"2021-12-08T21:16:26.349355Z","shell.execute_reply.started":"2021-12-08T21:16:22.816150Z","shell.execute_reply":"2021-12-08T21:16:26.348556Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"create the attention model","metadata":{"execution":{"iopub.status.busy":"2021-12-06T10:58:13.207447Z","iopub.execute_input":"2021-12-06T10:58:13.207779Z","iopub.status.idle":"2021-12-06T10:58:13.215052Z","shell.execute_reply.started":"2021-12-06T10:58:13.207742Z","shell.execute_reply":"2021-12-06T10:58:13.214284Z"}}},{"cell_type":"code","source":"repeator = RepeatVector(eng_length)\nconcatenator = Concatenate(axis=-1)\ndensor1 = Dense(128, activation = \"tanh\")\ndensor2 = Dense(1, activation = \"relu\")\nactivator = Activation('softmax', name='attention_weights') \ndotor = Dot(axes = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:26.350863Z","iopub.execute_input":"2021-12-08T21:16:26.351130Z","iopub.status.idle":"2021-12-08T21:16:26.393402Z","shell.execute_reply.started":"2021-12-08T21:16:26.351095Z","shell.execute_reply":"2021-12-08T21:16:26.392579Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def one_step_attention(a, s_prev):\n    s_prev = repeator(s_prev)\n    concat = concatenator([a,s_prev])\n    e = densor1(concat)\n    energies = densor2(e)\n    alphas = activator(energies)\n    context = dotor([alphas,a])\n    return context","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:26.398900Z","iopub.execute_input":"2021-12-08T21:16:26.401351Z","iopub.status.idle":"2021-12-08T21:16:26.408838Z","shell.execute_reply.started":"2021-12-08T21:16:26.401311Z","shell.execute_reply":"2021-12-08T21:16:26.408115Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"n_a = 128 \nn_s = 256 \npost_activation_LSTM_cell = LSTM(n_s, return_state = True) \noutput_layer = Dense(fr_vocab_size, activation='softmax')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:26.413766Z","iopub.execute_input":"2021-12-08T21:16:26.414196Z","iopub.status.idle":"2021-12-08T21:16:28.661843Z","shell.execute_reply.started":"2021-12-08T21:16:26.414167Z","shell.execute_reply":"2021-12-08T21:16:28.661096Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def modelf(Tx, Ty, n_a, n_s, input_vocab_size, output_vocab_size):\n    X = Input(shape=(Tx,))\n    X1 = Embedding(input_vocab_size, 512, mask_zero=True)(X)\n    s0 = Input(shape=(n_s,), name='s0')\n    c0 = Input(shape=(n_s,), name='c0')\n    s = s0\n    c = c0\n    outputs = []\n    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X1)\n    for t in range(Ty):\n        context = one_step_attention(a, s)\n        s, _, c = next_hidden_state, _ , next_cell_state = post_activation_LSTM_cell(inputs=context, initial_state=[s, c])\n        out =output_layer(s)\n        outputs.append(out)\n    outputs = tf.stack(outputs,axis=1)\n    model = Model(inputs=[X,s0,c0],outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:28.663256Z","iopub.execute_input":"2021-12-08T21:16:28.667753Z","iopub.status.idle":"2021-12-08T21:16:28.680134Z","shell.execute_reply.started":"2021-12-08T21:16:28.667711Z","shell.execute_reply":"2021-12-08T21:16:28.678800Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = modelf(eng_length,fr_length,n_a,n_s,eng_vocab_size, fr_vocab_size)\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy',metrics=['accuracy'])\nprint(model.summary())\nplot_model(model, to_file='model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:16:28.686248Z","iopub.execute_input":"2021-12-08T21:16:28.688717Z","iopub.status.idle":"2021-12-08T21:16:34.846753Z","shell.execute_reply.started":"2021-12-08T21:16:28.688659Z","shell.execute_reply":"2021-12-08T21:16:34.845322Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Train the model for 30 epochs","metadata":{"execution":{"iopub.status.busy":"2021-12-05T14:47:12.499849Z","iopub.execute_input":"2021-12-05T14:47:12.500573Z","iopub.status.idle":"2021-12-05T14:47:12.504726Z","shell.execute_reply.started":"2021-12-05T14:47:12.500533Z","shell.execute_reply":"2021-12-05T14:47:12.504011Z"}}},{"cell_type":"code","source":"s0 = np.zeros((trainX.shape[0], n_s))\nc0 = np.zeros((trainX.shape[0], n_s))\ncheckpoint = ModelCheckpoint('best_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit([trainX,s0,c0], trainY, epochs=30, batch_size=64, validation_split=0.1, callbacks=[checkpoint], verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:17:06.716752Z","iopub.execute_input":"2021-12-08T21:17:06.717035Z","iopub.status.idle":"2021-12-08T21:24:54.344879Z","shell.execute_reply.started":"2021-12-08T21:17:06.717001Z","shell.execute_reply":"2021-12-08T21:24:54.342159Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"make predictions on the test dataset","metadata":{}},{"cell_type":"code","source":"def word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:24:54.352618Z","iopub.execute_input":"2021-12-08T21:24:54.354137Z","iopub.status.idle":"2021-12-08T21:24:54.360268Z","shell.execute_reply.started":"2021-12-08T21:24:54.354103Z","shell.execute_reply":"2021-12-08T21:24:54.359447Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def predict_sequence(model, tokenizer, source):\n    s00 = np.zeros((1, n_s))\n    c00 = np.zeros((1, n_s))\n    prediction = model.predict([source,s00,c00], verbose=0)[0]\n    integers = [argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ' '.join(target)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:24:54.361813Z","iopub.execute_input":"2021-12-08T21:24:54.362193Z","iopub.status.idle":"2021-12-08T21:24:54.383475Z","shell.execute_reply.started":"2021-12-08T21:24:54.362156Z","shell.execute_reply":"2021-12-08T21:24:54.382536Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, tokenizer, sources, raw_dataset):\n    actual, predicted = list(), list()\n    for i, source in enumerate(sources):\n        source = source.reshape((1, source.shape[0]))\n        translation = predict_sequence(model, fr_tokenizer, source)\n        raw_src, raw_target = raw_dataset[i]\n        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n        actual.append([raw_target.split()])\n        predicted.append(translation.split())","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:24:54.385603Z","iopub.execute_input":"2021-12-08T21:24:54.385952Z","iopub.status.idle":"2021-12-08T21:24:54.395072Z","shell.execute_reply.started":"2021-12-08T21:24:54.385913Z","shell.execute_reply":"2021-12-08T21:24:54.394380Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model, fr_tokenizer, testX, test)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T21:24:54.396757Z","iopub.execute_input":"2021-12-08T21:24:54.397130Z","iopub.status.idle":"2021-12-08T21:25:05.639658Z","shell.execute_reply.started":"2021-12-08T21:24:54.397095Z","shell.execute_reply":"2021-12-08T21:25:05.638919Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}